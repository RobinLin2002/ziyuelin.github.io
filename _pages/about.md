---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I received my B.S. from The Chinese University of Hong Kong, Shenzhen. I am currently pursuing a master's degree in the Department of Mathematics at The University of Hong Kong, with a major in Artificial Intelligence. **I am currently seeking PhD positions for 2026 Fall.**

My research interests include computer vision, image restoration, and data-centric AI. This is my personal CV: [Ziyue Lin's Curriculum Vitae](../assets/æ—å­è¶Š_ä¸ªäººä¸­è‹±æ–‡ç®€å†.pdf)

I am very fortunate to be advised by [Prof.Liangqiong Qu](https://liangqiong.github.io/) from [School of Computing and Data Science](https://www.cds.hku.hk/), The University of Hong Kong. I am also currently collaborating with the EPIC Lab at Shanghai Jiao Tong University on research projects, and I am grateful for the guidance from Researcher [Shaobo Wang](https://gszfwsb.github.io/) and [Prof.Linfeng Zhang](http://www.zhanglinfeng.tech/).

æˆ‘æœ¬ç§‘æ¯•ä¸šäºé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰ï¼Œç›®å‰åœ¨é¦™æ¸¯å¤§å­¦æ•°å­¦ç³»æ”»è¯»äººå·¥æ™ºèƒ½æ–¹å‘çš„ç¡•å£«å­¦ä½ï¼Œå¹¶è®¡åˆ’ç”³è¯·2026å¹´ç§‹å­£å…¥å­¦çš„åšå£«é¡¹ç›®ã€‚

æˆ‘çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€å›¾åƒä¿®å¤ä»¥åŠæ•°æ®é©±åŠ¨çš„äººå·¥æ™ºèƒ½ã€‚è¿™æ˜¯æˆ‘çš„ä¸ªäººç®€å†ï¼š[ç®€å†](../assets/æ—å­è¶Š_ä¸ªäººä¸­è‹±æ–‡ç®€å†.pdf)

æˆ‘éå¸¸è£å¹¸èƒ½è·å¾—é¦™æ¸¯å¤§å­¦è®¡ç®—ä¸æ•°æ®ç§‘å­¦å­¦é™¢å±ˆé“ç¼æ•™æˆçš„æŒ‡å¯¼ï¼Œæˆ‘ç›®å‰è¿˜ä¸ä¸Šæµ·äº¤é€šå¤§å­¦EPICå®éªŒå®¤åˆä½œå¼€å±•ç§‘ç ”é¡¹ç›®ï¼Œæ„Ÿè°¢ç‹å°‘åšå­¦é•¿å’Œå¼ æ—å³°æ•™æˆçš„æŒ‡å¯¼ã€‚
# ğŸ”¥ News
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ FedVLMBench was submitted to Neurips-2025.
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ Started working on diffusion and image restoration.
- *2025.03*: &nbsp;ğŸ‰ğŸ‰ One paper was submitted to ICCV-2025.
- *2024.10*: &nbsp;ğŸ‰ğŸ‰ MLLM-Bench was accepted by NAACL-2025!
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ Started learning at HKU.

# ğŸ“ Publications & Pre-prints

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Submitted to Neurips 2025</div><img src='images/FedVLMBench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://www.arxiv.org/pdf/2506.09638)

Weiying Zheng, **Ziyue Lin**, Pengxin Guo, Yuyin Zhou, Feifei Wang, Liangqiong Qu

[**Project**](https://github.com/Arise-zwy/FedVLMBench) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Our work provides essential tools, datasets, and empirical guidance for the research community, offering a standardized platform to advance privacy-preserving, federated training of multimodal foundation models.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Rejected by ICCV 2025</div><img src='images/ARRA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment]([https://example.com](https://arxiv.org/pdf/2503.07334))

Xing Xie, Jiawei Liu, **Ziyue Lin**, Huijie Fan, Zhi Han, Yandong Tang, Liangqiong Qu

[**Project**](https://arxiv.org/abs/2503.07334) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Accepted by NAACL 2025</div><img src='images/MLLM-Bench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">



[MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V](https://aclanthology.org/2025.naacl-long.256.pdf)

Wentao Geâˆ—, Shunian Chenâˆ—, Guiming Hardy Chen*, Nuo Chen, Junying Chen, Zhihong Chenâ€ , Wenya Xie, Shuo Yan, Chenghao Zhu, **Ziyue Lin**, Dingjie Song, Xidong Wang, Anningzhe Gao, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wangâ€ 
    


[**Project**](https://github.com/FreedomIntelligence/MLLM-Bench) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We benchmark 21 popular MLLMs in a pairwise-comparison fashion, showing diverse performance across models.
</div>
</div>



# ğŸ“– Educations
- *2024.09 - 2025.07* (now)*, pursuing master in The University of Hong Kong, major in Artificial Intelligence 
- *2020.09 - 2024.06*, achieved Bachelor of Science in The Chinese University of Hong Kong, Shenzhen, major in Data Science and big data technology

